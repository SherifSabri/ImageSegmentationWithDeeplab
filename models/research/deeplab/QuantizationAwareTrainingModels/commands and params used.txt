


Frozen Graph Export:
python "${WORK_DIR}"/export_model.py \
  --logtostderr \
  --checkpoint_path="${CKPT_PATH}" \
  --export_path="${EXPORT_PATH}" \
  --model_variant="mobilenet_v2" \
  --num_classes=21 \
  --crop_size=513 \
  --crop_size=513 \
  --inference_scales=1.0\
  --quantize_delay_step=0   ## most important


Conversion:

tflite_convert   --graph_def_file=${OUTPUT_DIR}/frozen_inference_graph.pb   --output_file=${OUTPUT_DIR}/frozen_inference_graph.tflite   --output_format=TFLITE   --input_shape=1,513,513,3   --input_arrays="MobilenetV2/MobilenetV2/input"   --inference_type=QUANTIZED_UINT8   --inference_input_type=QUANTIZED_UINT8   --std_dev_values=128   --mean_values=128   --change_concat_input_ranges=true   --output_arrays="ArgMax"

